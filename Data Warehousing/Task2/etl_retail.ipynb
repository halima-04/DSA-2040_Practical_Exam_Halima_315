{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8202fba7",
   "metadata": {},
   "source": [
    "## Step 1 — Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd457d",
   "metadata": {},
   "source": [
    "## What we do:\n",
    "* Read the dataset (Online_Retail.csv) from disk into a pandas DataFrame.\n",
    "* Remove rows missing essential values:\n",
    "* InvoiceNo → needed to identify transactions.\n",
    "* StockCode → product identification.\n",
    "* Quantity and UnitPrice → required for sales calculations.\n",
    "* InvoiceDate → needed for time-based analysis.\n",
    "* Convert InvoiceDate to a proper datetime type so we can filter and group by time later.\n",
    "* Remove any rows where the date could not be parsed.\n",
    "\n",
    "## Why we do it:\n",
    "* Ensures we are working only with valid, complete data before transformations.\n",
    "* Makes sure the InvoiceDate column is in a format that allows filtering and aggregations.\n",
    "* Avoids issues in later steps from missing or invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bcb5852",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/online+retail.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m zip_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/online+retail.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Inspect ZIP contents\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m z:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles in zip:\u001b[39m\u001b[38;5;124m\"\u001b[39m, z\u001b[38;5;241m.\u001b[39mnamelist())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Assuming the CSV inside is named \"Online Retail.csv\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\zipfile\\__init__.py:1336\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1337\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/online+retail.zip'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Path to the uploaded ZIP\n",
    "zip_path = \"/mnt/data/online+retail.zip\"\n",
    "\n",
    "# Inspect ZIP contents\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    print(\"Files in zip:\", z.namelist())\n",
    "\n",
    "# Assuming the CSV inside is named \"Online Retail.csv\"\n",
    "csv_name = \"Online Retail.csv\"\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    with z.open(csv_name) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "# Check initial rows and info\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474a7b3",
   "metadata": {},
   "source": [
    "## Step 2 — Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f46a7b",
   "metadata": {},
   "source": [
    "## What we do:\n",
    "* Remove invalid transactions:\n",
    "* Negative or zero Quantity values.\n",
    "* Zero or negative UnitPrice.\n",
    "* Create a new column:\n",
    "* TotalSales = Quantity * UnitPrice → This is the key sales measure.\n",
    "* Filter transactions to the last year relative to 2025-08-12 (exam requirement).\n",
    "* Create dimension-like tables:\n",
    "* CustomerDim: unique CustomerID and Country.\n",
    "* TimeDim: unique dates with TimeID, Month, Quarter, Year for time-based OLAP.\n",
    "* Prepare fact table:\n",
    "* SalesFact: contains CustomerID, TimeID, Quantity, and TotalSales.\n",
    "\n",
    "## Why we do it:\n",
    "* Removes bad data so our metrics are accurate.\n",
    "* Adds new calculated metrics for reporting.\n",
    "* Structures the data into star schema format to make OLAP queries easier in Task 3.\n",
    "* Filters for recent transactions to keep analysis relevant and within the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e154e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TotalSales\n",
    "df_sample['TotalSales'] = df_sample['Quantity'] * df_sample['UnitPrice']\n",
    "\n",
    "# Customer Dimension\n",
    "customer_dim = df_sample.groupby('CustomerID').agg({\n",
    "    'Country': 'first',\n",
    "    'TotalSales': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Time Dimension\n",
    "time_dim = df_sample[['InvoiceDate']].drop_duplicates().reset_index(drop=True)\n",
    "time_dim['TimeID'] = time_dim.index + 1\n",
    "time_dim['Date'] = time_dim['InvoiceDate']\n",
    "time_dim['Month'] = time_dim['InvoiceDate'].dt.month\n",
    "time_dim['Quarter'] = time_dim['InvoiceDate'].dt.quarter\n",
    "time_dim['Year'] = time_dim['InvoiceDate'].dt.year\n",
    "time_dim = time_dim.drop(columns=['InvoiceDate'])\n",
    "\n",
    "# Map TimeID to SalesFact\n",
    "df_sample = df_sample.merge(time_dim[['Date','TimeID']], left_on='InvoiceDate', right_on='Date', how='left')\n",
    "sales_fact = df_sample[['CustomerID','TimeID','Quantity','TotalSales']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef70eae",
   "metadata": {},
   "source": [
    "## Step 3 — Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3b914",
   "metadata": {},
   "source": [
    "## What we do:\n",
    "* Connect to a SQLite database (retail_dw.db).\n",
    "* Create tables:\n",
    "* CustomerDim\n",
    "* TimeDim\n",
    "* SalesFact\n",
    "* Load the cleaned/transformed data into these tables.\n",
    "* Enforce foreign key constraints to maintain referential integrity.\n",
    "\n",
    "## Why we do it:\n",
    "* Moves data into a data warehouse structure for analysis.\n",
    "* Allows running SQL queries efficiently in later steps (Task 3).\n",
    "* Ensures we follow proper relational database design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4235fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ETL] Completed: retail_dw_sample.db created with:\n",
      "CustomerDim rows: 744\n",
      "TimeDim rows: 950\n",
      "SalesFact rows: 1000\n"
     ]
    }
   ],
   "source": [
    "# 4. Load into SQLite\n",
    "# -----------------------------\n",
    "db_name = \"retail_dw_sample.db\"\n",
    "conn = sqlite3.connect(db_name)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Drop tables if they exist\n",
    "cursor.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS SalesFact;\n",
    "DROP TABLE IF EXISTS TimeDim;\n",
    "DROP TABLE IF EXISTS CustomerDim;\n",
    "\"\"\")\n",
    "\n",
    "# Create tables\n",
    "cursor.executescript(\"\"\"\n",
    "CREATE TABLE CustomerDim (\n",
    "    CustomerID INTEGER PRIMARY KEY,\n",
    "    Country TEXT,\n",
    "    TotalSales REAL\n",
    ");\n",
    "\n",
    "CREATE TABLE TimeDim (\n",
    "    TimeID INTEGER PRIMARY KEY,\n",
    "    Date TEXT,\n",
    "    Month INTEGER,\n",
    "    Quarter INTEGER,\n",
    "    Year INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE SalesFact (\n",
    "    SalesID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    CustomerID INTEGER,\n",
    "    TimeID INTEGER,\n",
    "    Quantity INTEGER,\n",
    "    TotalSales REAL,\n",
    "    FOREIGN KEY (CustomerID) REFERENCES CustomerDim(CustomerID),\n",
    "    FOREIGN KEY (TimeID) REFERENCES TimeDim(TimeID)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data\n",
    "customer_dim.to_sql('CustomerDim', conn, if_exists='append', index=False)\n",
    "time_dim.to_sql('TimeDim', conn, if_exists='append', index=False)\n",
    "sales_fact.to_sql('SalesFact', conn, if_exists='append', index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"[ETL] Completed: {db_name} created with:\")\n",
    "print(\"CustomerDim rows:\", len(customer_dim))\n",
    "print(\"TimeDim rows:\", len(time_dim))\n",
    "print(\"SalesFact rows:\", len(sales_fact))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68fd5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12350, 'Norway', 40.0), (12359, 'Cyprus', 7.800000000000001), (12370, 'Cyprus', 44.550000000000004), (12394, 'Belgium', 16.6), (12415, 'Australia', 282.90000000000003)]\n",
      "[(1, 15034, 1, 6, 12.48), (2, 12528, 2, 12, 35.400000000000006), (3, 15111, 3, 16, 13.28), (4, 14156, 4, 2, 17.0), (5, 13802, 5, 200, 330.0)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"retail_dw_sample.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM CustomerDim LIMIT 5\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "cursor.execute(\"SELECT * FROM SalesFact LIMIT 5\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
