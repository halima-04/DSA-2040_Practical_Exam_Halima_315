{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8202fba7",
   "metadata": {},
   "source": [
    "## Step 1 — Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd457d",
   "metadata": {},
   "source": [
    "## What we do:\n",
    "* Read the dataset (Online_Retail.csv) from disk into a pandas DataFrame.\n",
    "* Remove rows missing essential values:\n",
    "* InvoiceNo → needed to identify transactions.\n",
    "* StockCode → product identification.\n",
    "* Quantity and UnitPrice → required for sales calculations.\n",
    "* InvoiceDate → needed for time-based analysis.\n",
    "* Convert InvoiceDate to a proper datetime type so we can filter and group by time later.\n",
    "* Remove any rows where the date could not be parsed.\n",
    "\n",
    "## Why we do it:\n",
    "* Ensures we are working only with valid, complete data before transformations.\n",
    "* Makes sure the InvoiceDate column is in a format that allows filtering and aggregations.\n",
    "* Avoids issues in later steps from missing or invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d1ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# === STEP 1: EXTRACT ===\n",
    "def extract(file_path):\n",
    "    \"\"\"\n",
    "    Reads CSV into DataFrame and cleans data.\n",
    "    \"\"\"\n",
    "    # Read CSV with encoding fix for £ symbol and other special chars\n",
    "    df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "    print(f\"[Extract] Raw rows read: {len(df)}\")\n",
    "\n",
    "    # Drop rows with missing critical fields\n",
    "    df = df.dropna(subset=[\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\"])\n",
    "\n",
    "    # Convert InvoiceDate to datetime\n",
    "    df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"InvoiceDate\"])  # Remove bad date rows\n",
    "\n",
    "    # Ensure numeric types\n",
    "    df[\"Quantity\"] = pd.to_numeric(df[\"Quantity\"], errors=\"coerce\")\n",
    "    df[\"UnitPrice\"] = pd.to_numeric(df[\"UnitPrice\"], errors=\"coerce\")\n",
    "\n",
    "    print(f\"[Extract] Rows after cleaning: {len(df)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474a7b3",
   "metadata": {},
   "source": [
    "## Step 2 — Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f46a7b",
   "metadata": {},
   "source": [
    "## What we do:\n",
    "* Remove invalid transactions:\n",
    "* Negative or zero Quantity values.\n",
    "* Zero or negative UnitPrice.\n",
    "* Create a new column:\n",
    "* TotalSales = Quantity * UnitPrice → This is the key sales measure.\n",
    "* Filter transactions to the last year relative to 2025-08-12 (exam requirement).\n",
    "* Create dimension-like tables:\n",
    "* CustomerDim: unique CustomerID and Country.\n",
    "* TimeDim: unique dates with TimeID, Month, Quarter, Year for time-based OLAP.\n",
    "* Prepare fact table:\n",
    "* SalesFact: contains CustomerID, TimeID, Quantity, and TotalSales.\n",
    "\n",
    "## Why we do it:\n",
    "* Removes bad data so our metrics are accurate.\n",
    "* Adds new calculated metrics for reporting.\n",
    "* Structures the data into star schema format to make OLAP queries easier in Task 3.\n",
    "* Filters for recent transactions to keep analysis relevant and within the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51da96b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    \"\"\"\n",
    "    Creates dimension and fact tables from cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add TotalSales column\n",
    "    df[\"TotalSales\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "\n",
    "    # Customer Dimension (ensure unique CustomerID)\n",
    "    customer_dim = (\n",
    "        df[[\"CustomerID\", \"Country\"]]\n",
    "        .drop_duplicates(subset=[\"CustomerID\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Time Dimension (ensure unique TimeID)\n",
    "    time_dim = df[[\"InvoiceDate\"]].drop_duplicates().reset_index(drop=True)\n",
    "    time_dim[\"TimeID\"] = range(1, len(time_dim) + 1)\n",
    "    time_dim[\"Date\"] = time_dim[\"InvoiceDate\"].dt.date\n",
    "    time_dim[\"Month\"] = time_dim[\"InvoiceDate\"].dt.month\n",
    "    time_dim[\"Quarter\"] = time_dim[\"InvoiceDate\"].dt.quarter\n",
    "    time_dim[\"Year\"] = time_dim[\"InvoiceDate\"].dt.year\n",
    "    time_dim = time_dim.drop(columns=[\"InvoiceDate\"])\n",
    "\n",
    "    # Map TimeID back to main dataframe\n",
    "    df = df.merge(\n",
    "        time_dim,\n",
    "        left_on=df[\"InvoiceDate\"].dt.date,\n",
    "        right_on=\"Date\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Sales Fact Table\n",
    "    sales_fact = df[[\"CustomerID\", \"TimeID\", \"Quantity\", \"TotalSales\"]].reset_index(drop=True)\n",
    "\n",
    "    print(f\"[Transform] CustomerDim: {len(customer_dim)} rows\")\n",
    "    print(f\"[Transform] TimeDim: {len(time_dim)} rows\")\n",
    "    print(f\"[Transform] SalesFact: {len(sales_fact)} rows\")\n",
    "\n",
    "    return customer_dim, time_dim, sales_fact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef70eae",
   "metadata": {},
   "source": [
    "## Step 3 — Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3b914",
   "metadata": {},
   "source": [
    "## What we do:\n",
    "* Connect to a SQLite database (retail_dw.db).\n",
    "* Create tables:\n",
    "* CustomerDim\n",
    "* TimeDim\n",
    "* SalesFact\n",
    "* Load the cleaned/transformed data into these tables.\n",
    "* Enforce foreign key constraints to maintain referential integrity.\n",
    "\n",
    "## Why we do it:\n",
    "* Moves data into a data warehouse structure for analysis.\n",
    "* Allows running SQL queries efficiently in later steps (Task 3).\n",
    "* Ensures we follow proper relational database design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9de780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(customer_dim, time_dim, sales_fact, db_name=\"retail_dw.db\"):\n",
    "    \"\"\"\n",
    "    Loads dimension and fact tables into SQLite database.\n",
    "    Drops existing tables before inserting new data.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop old tables if exist\n",
    "    cursor.executescript(\"\"\"\n",
    "    DROP TABLE IF EXISTS SalesFact;\n",
    "    DROP TABLE IF EXISTS TimeDim;\n",
    "    DROP TABLE IF EXISTS CustomerDim;\n",
    "    \"\"\")\n",
    "\n",
    "    # Create schema\n",
    "    cursor.executescript(\"\"\"\n",
    "    CREATE TABLE CustomerDim (\n",
    "        CustomerID INTEGER PRIMARY KEY,\n",
    "        Country TEXT\n",
    "    );\n",
    "    CREATE TABLE TimeDim (\n",
    "        TimeID INTEGER PRIMARY KEY,\n",
    "        Date TEXT,\n",
    "        Month INTEGER,\n",
    "        Quarter INTEGER,\n",
    "        Year INTEGER\n",
    "    );\n",
    "    CREATE TABLE SalesFact (\n",
    "        SalesID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        CustomerID INTEGER,\n",
    "        TimeID INTEGER,\n",
    "        Quantity INTEGER,\n",
    "        TotalSales REAL,\n",
    "        FOREIGN KEY (CustomerID) REFERENCES CustomerDim(CustomerID),\n",
    "        FOREIGN KEY (TimeID) REFERENCES TimeDim(TimeID)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert data\n",
    "    customer_dim.to_sql(\"CustomerDim\", conn, if_exists=\"append\", index=False)\n",
    "    time_dim.to_sql(\"TimeDim\", conn, if_exists=\"append\", index=False)\n",
    "    sales_fact.to_sql(\"SalesFact\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"[Load] Data loaded into {db_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf4bbcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Extract] Raw rows read: 541909\n",
      "[Extract] Rows after cleaning: 172782\n",
      "[Transform] CustomerDim: 3125 rows\n",
      "[Transform] TimeDim: 8774 rows\n",
      "[Transform] SalesFact: 13746099 rows\n",
      "[Load] Data loaded into retail_dw.db\n"
     ]
    }
   ],
   "source": [
    "# === RUN THE FULL ETL ===\n",
    "file_path = r\"C:\\Users\\Salma\\New folder\\OneDrive\\Desktop\\DSA 2040_Practical_Exam\\DSA-2040_Practical_Exam_Halima_315\\Online_Retail.csv\"\n",
    "df_extracted = extract(file_path)\n",
    "customer_dim, time_dim, sales_fact = transform(df_extracted)\n",
    "load(customer_dim, time_dim, sales_fact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
